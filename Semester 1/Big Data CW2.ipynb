{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Analysis - a review classification project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The way businesses connect with their clients has permanently changed as a result of social media. Thousands of tweets or reviews might be trending on social media in a matter of minutes. It would be extremely helpful for a company to be able to examine reviews in real time and discover the sentiment that underpins each one.\n",
    "\n",
    "Nowadays, a brand's internet reputation is one of its most precious assets. If a negative review or a blunder on social media is not addressed swiftly, it can be costly. Social media and review sentiment analysis helps a firm to keep track of what people are saying about it, its products, and services, as well as discover negative sentiment and the reasons for it.\n",
    "\n",
    "To deescalate the problem and minimize future unfavorable mentions, it's critical to spot negative patterns or irate consumers quickly.\n",
    "\n",
    "However, not only can social media/review sentiment analysis help with brand management, but it may also provide insight into customer preferences. Customers' ratings and opinions are incredibly valuable to businesses. \n",
    "\n",
    "Companies can use feedback to tailor their product or service to the preferences of their customers. People nowadays find it far more easy to tweet about their satisfaction or dissatisfaction with a service or product rather than leave a review on the company's website.\n",
    "\n",
    "Because social media posts and reviews are not designed to be well-written with a clear structure and a well summarized thought process, analyzing them can be difficult. Instead, reviews/posts are a relatively casual expression of an individual's thoughts.\n",
    "\n",
    "Furthermore, posts and reviews frequently contain spelling errors, making the process even more difficult. Finally, sentiment analysis/text classification allows a company to track its clients' emotions and comprehend how they feel. It adds a new dimension to the standard measures for analyzing brand performance and gives businesses new chances.\n",
    "\n",
    "Businesses might manually classify data by sentiment/ratings, but because the internet moves so quickly and thousands of customers can engage in minutes, this work must be automated. Review classification and sentiment analysis must be both quick and scalable in order to produce consistent, high-quality results.[3]\n",
    "\n",
    "Also businesses and especially Amazon are having trouble with fake reviews. On Amazon's marketplace, where products are rated on a five-star scale and a large number of positive reviews can help a brand stand out from a crowd of competitors, not everything is as it seems. Amazon has admitted to having a fake review problem as it tries to reign in coordinated activities on other websites to flood product listings with positive ratings in exchange for money, which is against the company's terms of service.\n",
    "\n",
    "The overflow of stars and comments — real and fraudulent — might be daunting for customers evaluating 15 variants of a product of their liking. \n",
    "\n",
    "When Amazon discovers that a seller has broken the rules, it bans them from the marketplace. It took down listings for Aukey and Mpow electronics in May after reports that the companies had engaged in paid review schemes.\n",
    "\n",
    "Amazon also claims to devote efforts to deleting fraudulent reviews and the accounts who post them, claiming that 200 million suspected phony reviews were removed before they were published in 2020.\n",
    "\n",
    "According to an Amazon spokesman, 99 percent of the company's actions on incentivized reviews are taken proactively, before concerns are identified. According to a spokeswoman, Amazon wants their costumers to shop with confidence, knowing that revies are real and genuine.\n",
    "\n",
    "However, because many businesses are anxious to outperform their competitors, buyers are unable to distinguish if a product's number of five-star reviews is genuine or falsely exaggerated. When faced with the prospect of dozens of knockoff items on an Amazon marketplace with over 2 million sellers worldwide, shoppers are unclear what to believe. Amazon also has a hard time distinguishing false reviews from real customers who have purchased and utilized a product. \n",
    "Their actions appear to be legitimate, and the same client may write both paid and unpaid evaluations.\n",
    "\n",
    "Fake reviews are frequently planned on social media sites that Amazon does not control, which is another huge difficulty for the corporation. A UK regulator warned in May that it will continue to investigate these Facebook and Instagram groups, noting that 16,000 social media groups that organized refunds for phony Amazon reviews had been removed.\n",
    "\n",
    "The business that owns those social media networks, Meta, prohibits the trading of reviews and has automatic systems in place to detect such schemes. People can report this type of group, and the firm will remove groups and content if they are deemed to be in violation of the regulations, according to the corporation. Amazon also keeps an eye on social media for groups that are coordinating reviews, and 6,000 of them were reported to social media companies last year.\n",
    "\n",
    "The issue is circular in nature. The more positive reviews a product receives, the more attention it gains as an Amazon \"best seller,\" and the faster it earns the trust of customers who have never purchased from the firm before. As the company's client base grows, so does the number of people from whom it may solicit paid evaluations, accelerating its ratings success even more.\n",
    "[1]-[2]\n",
    "\n",
    "\n",
    "        \n",
    "## Data\n",
    "\n",
    "Initially I wanted to do a Twitter sentiment analysis project with the 'Twitter US Airline Sentiment' Dataset found on Kaggle(https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment).\n",
    "\n",
    "However since this dataset is only 3 Megabytes big this was not suitable for a Big Data project. Instead I chose to use the 'Amazon Review Dataset(2018)'(https://nijianmo.github.io/amazon/index.html). \n",
    "\n",
    "Tis Dataset is an updated version of the Amazon review dataset released in 2014. As in the previous version, this dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).\n",
    "And there I downloaded the subsection 'Video Games' which has 2,565,349 reviews about video games.\n",
    "The data was downloaded and converted into a csv via a online converter. The csv has 1.34 gigabytes and was uploaded to the Hadoop File system under the name 'amazon.csv'. \n",
    "\n",
    "The video games subcategory is only a small part of many, so this project coulb be scaled up, by using more data from this dataset. I found the sample size if over 2 million reviews and 1.34gb sufficient for this project.\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "My hypothese is that it is possible to gather sentiment and review rating from text and summary of reviews. Of course, when someone leaves a review on amazon, there is no need to detect the rating, as the user would rate the product from 1 to 5 stars anway, so the need to gather sentiment and rewiew rating would be redundant.\n",
    "\n",
    "However, there are of course smaller ecommerce businesses who might not have a rating system in place. Also, there is no 1 to 5 star rating system on social media. If a user on twitter complains or praises a product on social media, there won't be a rating system in place.\n",
    "\n",
    "And nowadays it is way more likely for a user to publish his/her sentiment of a product on twitter or instagram instead of leaving a proper feedback on amazon or another website. So there is the need to gather sentiment from prodcut review texts and summarys.\n",
    "\n",
    "Furthermore and this  is by far the most challening task of this project, there is the need to detect fake reviews. My hypothese is that it is possible, to detect fake reviews to some extent.\n",
    "\n",
    "## Planned analysis\n",
    "\n",
    "I will try to create a machine learning model, which should predict the correct rating of a given review. Two iterations of this model will be tested: 1. predicting the correct rating from the review Text and 2. predicting the correct rating from the review summary, which is often much more concise. \n",
    "\n",
    "The last step is to try and identif fake reviews. The dataset has a column 'verified', which states wether the user who left the review was verified or not. Since amazon deletes fake reviews as soon as they detect it, this dataset might not contain a lot of fake reviews. Also there are no other datasets containing fake reviews. \n",
    "\n",
    "So by following the logic that a user who is not verified, might be more likely to leave a fake review, the machine learning model will try to predict wether a user was veriefied and therefore if the review was  fake or not.\n",
    "\n",
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import re\n",
    "import string\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import col,when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build spark session\n",
    "spark = SparkSession.builder.appName(\"amazonjm2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading file from HDFS path\n",
    "\n",
    "path = hdfs:///user/jmohr001/amazon.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the amazon.csv from HDFS\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs:///user/jmohr001/amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+----------------+--------------------+--------------------+--------------+----+-----+-----+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin|    reviewerName|          reviewText|             summary|unixReviewTime|vote|style|image|\n",
      "+-------+--------+-----------+--------------+----------+----------------+--------------------+--------------------+--------------+----+-----+-----+\n",
      "|    1.0|    True| 06 9, 2014|A21ROB4YDOZA5P|0439381673|   Mary M. Clark|I used to play th...|   Did not like this|    1402272000|null| null| null|\n",
      "|    3.0|    True|05 10, 2014|A3TNZ2Q5E7HTHD|0439381673|       Sarabatya|The game itself w...|      Almost Perfect|    1399680000|null| null| null|\n",
      "|    4.0|    True| 02 7, 2014|A1OKRM3QFEATQO|0439381673| Amazon Customer|I had to learn th...|DOES NOT WORK WIT...|    1391731200|  15| null| null|\n",
      "|    1.0|    True| 02 7, 2014|A2XO1JFCNEYV3T|0439381673|ColoradoPartyof5|The product descr...|does not work on ...|    1391731200|  11| null| null|\n",
      "|    4.0|    True|01 16, 2014|A19WLPIRHD15TH|0439381673|  Karen Robinson|I would recommend...|                null|          null|null| null| null|\n",
      "+-------+--------+-----------+--------------+----------+----------------+--------------------+--------------------+--------------+----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# insepct data\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['overall',\n",
       " 'verified',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'asin',\n",
       " 'reviewerName',\n",
       " 'reviewText',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'vote',\n",
       " 'style',\n",
       " 'image']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-model with summary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             overall|             summary|\n",
      "+--------------------+--------------------+\n",
      "|                 1.0|   Did not like this|\n",
      "|                 3.0|      Almost Perfect|\n",
      "|                 4.0|DOES NOT WORK WIT...|\n",
      "|                 1.0|does not work on ...|\n",
      "|                 4.0|                null|\n",
      "|I really like pla...|                null|\n",
      "|                 5.0|Love this game!  ...|\n",
      "|                 3.0|Would like it mor...|\n",
      "|                 5.0|                null|\n",
      "|               [...]|                null|\n",
      "|Just the patch al...|                null|\n",
      "|Classic game that...|                null|\n",
      "|                 5.0|The Oregon Trail-...|\n",
      "|                 5.0|                null|\n",
      "|I got this game w...|                null|\n",
      "|Graphics: The gra...|                null|\n",
      "|Sound: The sound ...|                null|\n",
      "|Pros: Help ppl le...|                null|\n",
      "|Is really fun to ...|                null|\n",
      "|Has really cool s...|                null|\n",
      "|Cons: For some re...|                null|\n",
      "|Can sometimes be ...|                null|\n",
      "|The disc got scra...|                null|\n",
      "|Overall it's a go...|                null|\n",
      "|                 5.0|           Still fun|\n",
      "+--------------------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get only relevant columns\n",
    "df = data.select('overall','summary')\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              rating|             summary|\n",
      "+--------------------+--------------------+\n",
      "|                 1.0|   Did not like this|\n",
      "|                 3.0|      Almost Perfect|\n",
      "|                 4.0|DOES NOT WORK WIT...|\n",
      "|                 1.0|does not work on ...|\n",
      "|                 4.0|                null|\n",
      "|I really like pla...|                null|\n",
      "|                 5.0|Love this game!  ...|\n",
      "|                 3.0|Would like it mor...|\n",
      "|                 5.0|                null|\n",
      "|               [...]|                null|\n",
      "|Just the patch al...|                null|\n",
      "|Classic game that...|                null|\n",
      "|                 5.0|The Oregon Trail-...|\n",
      "|                 5.0|                null|\n",
      "|I got this game w...|                null|\n",
      "|Graphics: The gra...|                null|\n",
      "|Sound: The sound ...|                null|\n",
      "|Pros: Help ppl le...|                null|\n",
      "|Is really fun to ...|                null|\n",
      "|Has really cool s...|                null|\n",
      "|Cons: For some re...|                null|\n",
      "|Can sometimes be ...|                null|\n",
      "|The disc got scra...|                null|\n",
      "|Overall it's a go...|                null|\n",
      "|                 5.0|           Still fun|\n",
      "+--------------------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chaning column names\n",
    "newcolnames = ['rating','summary']\n",
    "\n",
    "for c,n in zip(df.columns,newcolnames):\n",
    "    df=df.withColumnRenamed(c,n)\n",
    "    \n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dtype to int for rating column\n",
    "df = df.withColumn(\"rating\",df.rating.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|             summary|\n",
      "+------+--------------------+\n",
      "|     1|   Did not like this|\n",
      "|     3|      Almost Perfect|\n",
      "|     4|DOES NOT WORK WIT...|\n",
      "|     1|does not work on ...|\n",
      "|     5|Love this game!  ...|\n",
      "|     3|Would like it mor...|\n",
      "|     5|The Oregon Trail-...|\n",
      "|     5|           Still fun|\n",
      "|     5|         Fun to Play|\n",
      "|     1|           Cant play|\n",
      "|     1| NOT FOR A newer MAC|\n",
      "|     5| A must have game!!!|\n",
      "|     5|          Five Stars|\n",
      "|     5|          Five Stars|\n",
      "|     2|   Very disappointed|\n",
      "|     5|          Five Stars|\n",
      "|     5|Believe it is bei...|\n",
      "|     4|          Four Stars|\n",
      "|     3|            So. Old.|\n",
      "|     5|          Still Fun!|\n",
      "|     1|       what a waste.|\n",
      "|     1|        Disappointed|\n",
      "|     1|I have been unabl...|\n",
      "|     4| Blast from the Past|\n",
      "|     1|... work in my co...|\n",
      "+------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop row  if  null value  is in row\n",
    "df = df.na.drop()\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     5|1284099|\n",
      "|     4| 309921|\n",
      "|     1| 254602|\n",
      "|     3| 157666|\n",
      "|     2| 103753|\n",
      "|  1985|      4|\n",
      "|   360|      2|\n",
      "|    10|      2|\n",
      "|     6|      1|\n",
      "|    -1|      1|\n",
      "|   552|      1|\n",
      "|     7|      1|\n",
      "|   343|      1|\n",
      "|   100|      1|\n",
      "|    20|      1|\n",
      "|   250|      1|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby rating\n",
    "df.groupBy(\"rating\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|             summary|\n",
      "+------+--------------------+\n",
      "|     1|   Did not like this|\n",
      "|     3|      Almost Perfect|\n",
      "|     4|DOES NOT WORK WIT...|\n",
      "|     1|does not work on ...|\n",
      "|     5|Love this game!  ...|\n",
      "|     3|Would like it mor...|\n",
      "|     5|The Oregon Trail-...|\n",
      "|     5|           Still fun|\n",
      "|     5|         Fun to Play|\n",
      "|     1|           Cant play|\n",
      "|     1| NOT FOR A newer MAC|\n",
      "|     5| A must have game!!!|\n",
      "|     5|          Five Stars|\n",
      "|     5|          Five Stars|\n",
      "|     2|   Very disappointed|\n",
      "|     5|          Five Stars|\n",
      "|     5|Believe it is bei...|\n",
      "|     4|          Four Stars|\n",
      "|     3|            So. Old.|\n",
      "|     5|          Still Fun!|\n",
      "|     1|       what a waste.|\n",
      "|     1|        Disappointed|\n",
      "|     1|I have been unabl...|\n",
      "|     4| Blast from the Past|\n",
      "|     1|... work in my co...|\n",
      "+------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter only the 1-5 star ratings\n",
    "df = df.filter((df.rating == '5.0') | (df.rating == '4.0') | (df.rating == '3.0') | (df.rating == '2.0') | (df.rating == '1.0'))\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     5|1284099|\n",
      "|     4| 309921|\n",
      "|     1| 254602|\n",
      "|     3| 157666|\n",
      "|     2| 103753|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby rating and see if only ratrings from 1-5 are there\n",
    "df.groupBy(\"rating\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|summary|\n",
      "+------+-------+\n",
      "|     0|      0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#looking for missing values in the data\n",
    "df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lowercasing text\n",
    "def lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for seperating phrases like can't into can not\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing punctuation\n",
    "def process_text_basic(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for filtering empty string\n",
    "def empty_string(text):\n",
    "    filter(None, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making udf functions out of normal python functions\n",
    "\n",
    "# convert text to lowercase\n",
    "lower_case_udf=F.udf(f=lambda row: lower_case(row), returnType=T.StringType())\n",
    "\n",
    "# decontract phrases like can't -> can not\n",
    "decontracted_udf=F.udf(f=lambda row: decontracted(row), returnType=T.StringType())\n",
    "\n",
    "# process text function, basically only removing punctuation\n",
    "process_text_basic_udf=F.udf(f=lambda row: process_text_basic(row), returnType=T.StringType())\n",
    "\n",
    "# empty string function\n",
    "empty_string_udf=F.udf(f=lambda row: empty_string(row), returnType=T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying udf functions to df\n",
    "\n",
    "df = df.withColumn(\"summary\",lower_case_udf(F.col(\"summary\")))\n",
    "\n",
    "df = df.withColumn(\"summary\",decontracted_udf(F.col(\"summary\")))\n",
    "\n",
    "df = df.withColumn(\"summary\",process_text_basic_udf(F.col(\"summary\")))\n",
    "\n",
    "df = df.withColumn(\"summary\",empty_string_udf(F.col(\"summary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where empty string replace with none\n",
    "df=df.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "# drop none values\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and testing datasets\n",
    "train_df,test_df = df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|             summary|\n",
      "+------+--------------------+\n",
      "|     1|  i used to enjoy...|\n",
      "|     1|  it is terrible ...|\n",
      "|     1|  keep on dreamin...|\n",
      "|     1|        we need wood|\n",
      "|     1|  you hit a butto...|\n",
      "|     1|          128meg ram|\n",
      "|     1|   18 wheeler racing|\n",
      "|     1| 2001 we should s...|\n",
      "|     1| 2gb ram w 256mb ...|\n",
      "|     1| 5 headshots with...|\n",
      "|     1| 9800 radian vide...|\n",
      "|     1| a day which will...|\n",
      "|     1| a game using the...|\n",
      "|     1| a helpless consumer|\n",
      "|     1| a lot more fight...|\n",
      "|     1| a message pops u...|\n",
      "|     1| a new patch came...|\n",
      "|     1| a physical pain ...|\n",
      "|     1| a term i use to ...|\n",
      "|     1| a timer starts t...|\n",
      "|     1| accounts for app...|\n",
      "|     1|           after all|\n",
      "|     1| after reaching d...|\n",
      "|     1| after that come ...|\n",
      "|     1| alexi123 and man...|\n",
      "+------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 25 entries of train_df\n",
    "train_df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|rating| count|\n",
      "+------+------+\n",
      "|     5|962076|\n",
      "|     4|232554|\n",
      "|     1|190590|\n",
      "|     3|118564|\n",
      "|     2| 77596|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if everything went right, when splitting into training data\n",
    "train_df.groupBy(\"rating\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SparkMLlib to develop machine learing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  stages for the pipeline \n",
    "tokenizer = Tokenizer(inputCol='summary',outputCol='tokens')\n",
    "stopwords_remover= StopWordsRemover(inputCol='tokens',outputCol='filtered_tokens')\n",
    "vectorizer=CountVectorizer(inputCol='filtered_tokens',outputCol='features')\n",
    "idf = IDF(inputCol='features',outputCol='vectorized_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logisit regression estimator\n",
    "lr = LogisticRegression(featuresCol='vectorized_features',labelCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model\n",
    "lr_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on test dataset\n",
    "predictions = lr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rating',\n",
       " 'summary',\n",
       " 'tokens',\n",
       " 'filtered_tokens',\n",
       " 'features',\n",
       " 'vectorized_features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select columns\n",
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------+\n",
      "|             summary|rating|prediction|\n",
      "+--------------------+------+----------+\n",
      "|  but that was li...|     1|       5.0|\n",
      "|  chessmaster 800...|     1|       1.0|\n",
      "| 15 bucks but the...|     1|       1.0|\n",
      "| 2 minutes rounds...|     1|       3.0|\n",
      "| 98 which is neve...|     1|       1.0|\n",
      "| a baldur is gate...|     1|       5.0|\n",
      "| a nfl blitz styl...|     1|       5.0|\n",
      "| a scam it does n...|     1|       1.0|\n",
      "| a serious error ...|     1|       1.0|\n",
      "| acof ran like a ...|     1|       1.0|\n",
      "| after spending a...|     1|       2.0|\n",
      "|               again|     1|       5.0|\n",
      "| all look like th...|     1|       5.0|\n",
      "| and 4 wide but i...|     1|       4.0|\n",
      "|      and a snickers|     1|       5.0|\n",
      "| and boy does it ...|     1|       3.0|\n",
      "| and cause gamepl...|     1|       2.0|\n",
      "| and doing ridicu...|     1|       1.0|\n",
      "| and i have heard...|     1|       5.0|\n",
      "| and i liked the ...|     1|       5.0|\n",
      "| and i was very s...|     1|       5.0|\n",
      "| and in large groups|     1|       5.0|\n",
      "| and it all would...|     1|       3.0|\n",
      "| and it will not ...|     1|       2.0|\n",
      "| and may yet be a...|     1|       5.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show summary rating and prediction\n",
    "predictions.select('summary','rating','prediction').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model evaluation\n",
    "evaluator=MulticlassClassificationEvaluator(labelCol='rating',predictionCol='prediction',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7443476478814709\n"
     ]
    }
   ],
   "source": [
    "# model accuracy on summary column\n",
    "accuracy_summarycol = evaluator.evaluate(predictions)\n",
    "print(accuracy_summarycol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! An accuracy of almost 75% has been achieved. Considering that there are 5 labels the model has to correctly classify the given text to, this is a very good result. The review summary gives a good indication of wether a review was good or bad, as it is supposed to be concise. Let's see if the model can achieve the same accuarcy with the text column as input.\n",
    "\n",
    "### ML-model with text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             overall|          reviewText|\n",
      "+--------------------+--------------------+\n",
      "|                 1.0|I used to play th...|\n",
      "|                 3.0|The game itself w...|\n",
      "|                 4.0|I had to learn th...|\n",
      "|                 1.0|The product descr...|\n",
      "|                 4.0|I would recommend...|\n",
      "|I really like pla...|                null|\n",
      "|                 5.0|Choose your caree...|\n",
      "|                 3.0|Would like it mor...|\n",
      "|                 5.0|It took a few hou...|\n",
      "|               [...]|                null|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating new df from original data\n",
    "df2 = data.select('overall','reviewText')\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaning column names\n",
    "newcolnames = ['rating','text']\n",
    "\n",
    "for c,n in zip(df2.columns,newcolnames):\n",
    "    df2=df2.withColumnRenamed(c,n)\n",
    "\n",
    "# filtering only rated reviews\n",
    "df2 = df2.filter((df2.rating == '5.0') | (df2.rating == '4.0') | (df2.rating == '3.0') | (df2.rating == '2.0') | (df2.rating == '1.0'))\n",
    "\n",
    "df2 = df2.withColumn(\"rating\",df2.rating.cast('int'))\n",
    "\n",
    "# drop row  if  null value  is in row\n",
    "df2 = df2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     5|1485936|\n",
      "|     4| 412278|\n",
      "|     1| 311808|\n",
      "|     3| 212302|\n",
      "|     2| 141306|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby rating and see if only ratrings from 1-5 are there\n",
    "df2.groupBy(\"rating\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying udf functions to df\n",
    "\n",
    "df2 = df2.withColumn(\"text\",lower_case_udf(F.col(\"text\")))\n",
    "\n",
    "df2 = df2.withColumn(\"text\",decontracted_udf(F.col(\"text\")))\n",
    "\n",
    "df2 = df2.withColumn(\"text\",process_text_basic_udf(F.col(\"text\")))\n",
    "\n",
    "df2 = df2.withColumn(\"text\",empty_string_udf(F.col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where empty string replace with none\n",
    "df2=df2.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in df2.columns])\n",
    "\n",
    "# drop none values\n",
    "df2 = df2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and testing datasets\n",
    "train_df2,test_df2 = df2.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  stages for the pipeline \n",
    "tokenizer = Tokenizer(inputCol='text',outputCol='tokens')\n",
    "stopwords_remover= StopWordsRemover(inputCol='tokens',outputCol='filtered_tokens')\n",
    "vectorizer=CountVectorizer(inputCol='filtered_tokens',outputCol='features')\n",
    "idf = IDF(inputCol='features',outputCol='vectorized_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logisit regression estimator\n",
    "lr = LogisticRegression(featuresCol='vectorized_features',labelCol='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model\n",
    "lr_model = pipeline.fit(train_df2)\n",
    "\n",
    "# predictions on test dataset\n",
    "predictions = lr_model.transform(test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------+\n",
      "|                text|rating|prediction|\n",
      "+--------------------+------+----------+\n",
      "|  it is too small...|     1|       1.0|\n",
      "| i am quite sorry...|     1|       5.0|\n",
      "| noty the tasmani...|     1|       1.0|\n",
      "| the cool box art...|     1|       5.0|\n",
      "| the graphics are...|     1|       5.0|\n",
      "|1 ai not n way no...|     1|       5.0|\n",
      "|18900 are you kid...|     1|       5.0|\n",
      "|1who does this qu...|     1|       2.0|\n",
      "|2 days after arri...|     1|       1.0|\n",
      "|2 player does not...|     1|       1.0|\n",
      "|29940 for only th...|     1|       5.0|\n",
      "|4 short games 50 ...|     1|       1.0|\n",
      "|a 60fps original ...|     1|       3.0|\n",
      "|a bit of a snoore...|     1|       5.0|\n",
      "|a datahookproduct...|     1|       5.0|\n",
      "|a datahookproduct...|     1|       3.0|\n",
      "|a few words befor...|     1|       5.0|\n",
      "|a friend lent me ...|     1|       5.0|\n",
      "|a fun game to pla...|     1|       1.0|\n",
      "|a great looking g...|     1|       1.0|\n",
      "|a longtime fan of...|     1|       1.0|\n",
      "|a lot of good rev...|     1|       5.0|\n",
      "|a lot of survival...|     1|       1.0|\n",
      "|a major disappoin...|     1|       3.0|\n",
      "|a month or so ago...|     1|       1.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('text','rating','prediction').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model evaluation\n",
    "evaluator=MulticlassClassificationEvaluator(labelCol='rating',predictionCol='prediction',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6550704475768342\n"
     ]
    }
   ],
   "source": [
    "# model accuracy on summary column\n",
    "accuracy_textcol = evaluator.evaluate(predictions)\n",
    "print(accuracy_textcol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the accuarcy decreased. The review texts are longer and more complicated than the summary. 65% is still decent though since this is a mutliclass classification task.\n",
    "\n",
    "At last, fake reviews will be predicted.\n",
    "\n",
    "### ML-model verified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new df from original data\n",
    "df3 = data.select('overall','summary','verified')\n",
    "\n",
    "# chaning column names\n",
    "newcolnames = ['rating','summary','verified']\n",
    "\n",
    "for c,n in zip(df3.columns,newcolnames):\n",
    "    df3=df3.withColumnRenamed(c,n)\n",
    "\n",
    "# filtering only rated reviews\n",
    "df3 = df3.filter((df3.rating == '5.0') | (df3.rating == '4.0') | (df3.rating == '3.0') | (df3.rating == '2.0') | (df3.rating == '1.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|verified|  count|\n",
      "+--------+-------+\n",
      "|    True|1948309|\n",
      "|   False| 617040|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby verified and see \n",
    "df3.groupBy(\"verified\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding labels:  If review is true = 0, if false=1\n",
    "df3=df3.select([when(col(c)==\"True\",0).otherwise(col(c)).alias(c) for c in df3.columns])\n",
    "df3=df3.select([when(col(c)==\"False\",1).otherwise(col(c)).alias(c) for c in df3.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where empty string replace with none\n",
    "df3=df3.select([when(col(c)==\"\",None).otherwise(col(c)).alias(c) for c in df3.columns])\n",
    "\n",
    "# drop none values\n",
    "df3 = df3.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying udf functions to df\n",
    "\n",
    "df3 = df3.withColumn(\"summary\",lower_case_udf(F.col(\"summary\")))\n",
    "\n",
    "df3 = df3.withColumn(\"summary\",decontracted_udf(F.col(\"summary\")))\n",
    "\n",
    "df3 = df3.withColumn(\"summary\",process_text_basic_udf(F.col(\"summary\")))\n",
    "\n",
    "df3 = df3.withColumn(\"summary\",empty_string_udf(F.col(\"summary\")))\n",
    "\n",
    "df3 = df3.withColumn(\"verified\",empty_string_udf(F.col(\"verified\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting verified labels to integer dtype\n",
    "df3 = df3.withColumn(\"verified\",df3.rating.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and testing datasets\n",
    "train_df3,test_df3 = df3.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stages for the pipeline \n",
    "tokenizer = Tokenizer(inputCol='summary',outputCol='tokens')\n",
    "stopwords_remover= StopWordsRemover(inputCol='tokens',outputCol='filtered_tokens')\n",
    "vectorizer=CountVectorizer(inputCol='filtered_tokens',outputCol='features')\n",
    "idf = IDF(inputCol='features',outputCol='vectorized_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,stopwords_remover,vectorizer,idf,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logisit regression estimator\n",
    "lr = LogisticRegression(featuresCol='vectorized_features',labelCol='verified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model\n",
    "lr_model = pipeline.fit(train_df3)\n",
    "\n",
    "# predictions on test dataset\n",
    "predictions = lr_model.transform(test_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+----------+\n",
      "|             summary|rating|verified|prediction|\n",
      "+--------------------+------+--------+----------+\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|                    |   1.0|       1|       5.0|\n",
      "|        we need wood|   1.0|       1|       5.0|\n",
      "|          128meg ram|   1.0|       1|       5.0|\n",
      "| a day which will...|   1.0|       1|       4.0|\n",
      "| a new patch came...|   1.0|       1|       1.0|\n",
      "| a scam it does n...|   1.0|       1|       1.0|\n",
      "|           after all|   1.0|       1|       5.0|\n",
      "| after reaching d...|   1.0|       1|       1.0|\n",
      "| alexi123 and man...|   1.0|       1|       1.0|\n",
      "| alot of people c...|   1.0|       1|       5.0|\n",
      "| an actual rpg fo...|   1.0|       1|       1.0|\n",
      "| an extremely clu...|   1.0|       1|       2.0|\n",
      "| and 4 wide but i...|   1.0|       1|       4.0|\n",
      "+--------------------+------+--------+----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('summary','rating','verified','prediction').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model evaluation\n",
    "evaluator=MulticlassClassificationEvaluator(labelCol='verified',predictionCol='prediction',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7440916238798052\n"
     ]
    }
   ],
   "source": [
    "# model accuracy on verified column\n",
    "accuracy_fakereview = evaluator.evaluate(predictions)\n",
    "print(accuracy_fakereview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again almost 75%! This result is hard to interpret, because there are probably no fake reviews in the dataset. So what makes a review from a unverified user different, than from a verified one? Nothing really. Nonetheless, efforts have been made to classify fake reviews.\n",
    "\n",
    "\n",
    "# Summary and Conclusions\n",
    "\n",
    "At first I had to load the data from HDFS. Afterwards there was the need to inspect the data and explore the columns. \n",
    "For a task like this, it is necessary to know, which columns need to be used and which are irrelevant to the project.\n",
    "\n",
    "By looking at the data via df.show() I was able to see that there was a lot of noise in the data. With datasets as big as this one, it is very common to have noise in data which needs to be cleaned up.\n",
    "\n",
    "In the process of cleaning, I changed the column names of the updated dataframe to more relevant names. Furthermore the datatypes of the columns were adjusted so numbers were actuatlly integers instead of strings.\n",
    "\n",
    "Then I filtered only the relevant ratings with the scores 1 to 5, because there was other rubbish in the rating column not 1 to 5. \n",
    "\n",
    "With the groupby function I always confirmed manually if everything was filtered the right way. Afterwards NaN or missing values were removed from the dataframe.\n",
    "\n",
    "Now with a text classifcation task like this, it is from utmost importance that the text itself is cleaned properly.\n",
    "That is why i implemented 4 typical python functions to preprocess the text:\n",
    "\n",
    "    1. The decontracted function which pulls phrases like can't into can not\n",
    "    2. lowercase function for making the whole input text lowercase\n",
    "    3. process_text_basic function for removing punctuation\n",
    "    4. and the empty_string function, which removes text only including empty strings\n",
    "    \n",
    "Those functions were implemented via the user defined function(udf) from pyspark.\n",
    "\n",
    "Preprocessing text like this is important because in the ML-pipeline, the text will be tokenized and then tf-idf will be applied which means the tokens will be counted by term frequency and inverse document frequency. So it is necessary to process similar phrases into identical tokens. For example: \"I CAN'T\" and \"i can not\" would be 6 different tokens but they say the exact same thing, so the functions mentioned above take care of this problem, so that both phrases have the same exact three tokens like 'i' 'can' 'not.\n",
    "\n",
    "After the the text/summary preprocessing, the dataframe has been split into training and testing parts. This is needed to train the model on training data and then test it on unseen data. Then, the ML-Pipline consisting of tokenizer, stopwords_remover, vectorizer and idf was deployed. The stopwords remover, is an important part of this pipeline as it filters frequent words without any meaning towards sentiment. The most common stop words are pronouns, articles, prepositions, and conjunctions. This includes words like a, an, the, and, it, for, or, but, in, my, your, our, and their. In this pipeline the tokenized and filtered text is vectorized as the machine learning model can't work with words but with arrays of numbers. \n",
    "\n",
    "I used logistig regression for this task. The model was trained and then tested on the test set. The predictions were shown and the accuarcy of corrected predicted labels to total labels was calcuclated. \n",
    "\n",
    "This whole process was repeated three times for each set of objective in this project:\n",
    "    \n",
    "    1. For the summary column and rating classification\n",
    "    2. For the text column and rating classification\n",
    "    3. For the verified column to predict wehter a review was verified\n",
    "    \n",
    "The results were the following:\n",
    "\n",
    "    1.~75% accuarcy of correctly predicted ratings for a given summary review text\n",
    "    2.~65% accuarcy of correctly predicted ratings for a given complete review text\n",
    "    3.~75% accuarcy of correctly predicted wether the summary text belonged to a verified user or not \n",
    "    \n",
    "Since this dataset contained well above 250.000 reviews after cleaning, and the tasks for the first 2 objective were multilabel classification the results were impressive! It is not that easy to differentiate between a 2 and 3 star rating review for example. And thats exactly the type of work the model had to carry out.\n",
    "\n",
    "So my hypothesis that a machine learning model can accuartly predict ratings and therefore sentiment of a given text is true.\n",
    "\n",
    "As the commonsense baseline for this task would be 20% accaurcy(5 labels, 20% chance to guess it right randomly), the model beat that baseline handily each time. \n",
    "\n",
    "The fake review/verified user task was a little bit unclear, because as I said earlier, there are proably not a lot of fake reives in this dataset, if so, my theory that they all belong to unverifed users might not be true. However, the accaurcy of detecting wether a review text came from a verified source or not, was actually impressive aswell.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "[1] https://www.cnet.com/tech/services-and-software/features/amazons-never-ending-fake-reviews-problem-explained/\n",
    "\n",
    "[2] https://www.aboutamazon.com/news/how-amazon-works/creating-a-trustworthy-reviews-experience?tag=cnet-buy-button-20&ascsubtag=4066e7bd-1a2e-46f1-969f-f45464fec7dc%7C%7Cdtp\n",
    "\n",
    "[3] https://monkeylearn.com/sentiment-analysis/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
